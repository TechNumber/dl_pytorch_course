{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vit Transformer"
   ],
   "metadata": {
    "id": "mfCxjGV5oTiJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Модель"
   ],
   "metadata": {
    "collapsed": false,
    "id": "D-FMYv-jwLiV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3hL_6WoCwLiX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "tensor([[ 0.5724, -0.6533,  1.1049, -0.4849, -1.7450,  1.1229, -0.9716,  0.7787,\n",
      "          1.1459, -0.2301],\n",
      "        [-0.2087, -0.3661, -0.7727,  1.9516,  1.5495,  1.6531, -1.0622,  0.5459,\n",
      "          0.2340, -1.3714],\n",
      "        [ 0.1229, -1.1388,  0.8286,  1.2500, -0.6799,  1.2717,  2.5585,  0.5955,\n",
      "         -0.0385, -0.9558],\n",
      "        [-0.9188,  1.2414,  0.1035, -0.8419,  1.6002, -0.3624,  0.7136, -0.1262,\n",
      "         -0.9676, -1.4855],\n",
      "        [-1.1576,  0.6840, -0.5990, -1.0105,  0.4938, -0.5943,  1.1988, -0.7912,\n",
      "          1.4762,  0.3026]])\n"
     ]
    }
   ],
   "source": [
    "# Смоделируем данные\n",
    "\n",
    "n_features = 10  # Количество признаков\n",
    "n_classes = 3  # Количество классов\n",
    "batch_size = 5 \n",
    "\n",
    "data = torch.randn((batch_size, n_features))\n",
    "print(data.shape)\n",
    "print(data)"
   ],
   "metadata": {
    "id": "GVgNwDyzwLiY",
    "outputId": "e17b860e-2656-4ee2-bab7-9c96cf50d525"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Зададим простую модель\n",
    "model = nn.Linear(n_features, n_classes)"
   ],
   "metadata": {
    "id": "GKVgvWaawLiZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[-0.1430, -0.1978, -0.2459],\n",
      "        [ 0.0568,  0.2415,  1.2328],\n",
      "        [ 0.3432,  0.4882, -0.8710],\n",
      "        [-0.0182,  0.0482,  0.2527],\n",
      "        [ 0.3254, -0.0420,  0.0331]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Применим модель к вектору\n",
    "answer = model(data)\n",
    "print(answer.shape)\n",
    "print(answer)"
   ],
   "metadata": {
    "id": "PyfujdNcwLia",
    "outputId": "69aeff39-f7f4-4956-e5d4-652d7d10f290"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Модель как наследник nn.Module\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)"
   ],
   "metadata": {
    "id": "oCsGwzuRwLib"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[ 0.5724,  0.0264, -0.9001],\n",
      "        [ 0.3813,  0.7960, -0.4653],\n",
      "        [ 0.2878, -0.9297, -0.8761],\n",
      "        [-0.4443, -0.0975, -0.2454],\n",
      "        [-0.1670, -0.1594,  0.1794]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Попробуем применить модель в виде класса к данным\n",
    "model = SimpleNN(n_features, n_classes)\n",
    "\n",
    "answer = model(data)\n",
    "print(answer.shape)\n",
    "print(answer)"
   ],
   "metadata": {
    "id": "AJvKzV5ewLic",
    "outputId": "239a06aa-d46b-47df-9ac9-5fe59fc61bb2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/yessense/PycharmProjects/scene_vae/venv/lib/python3.8/site-packages (1.5.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/yessense/PycharmProjects/scene_vae/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 5, 3]              33\n",
      "================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "model = SimpleNN(n_features, n_classes).cuda()\n",
    "\n",
    "# 5, 10\n",
    "input_size = (batch_size, n_features)\n",
    "print(summary(model, input_size))"
   ],
   "metadata": {
    "id": "3I-SqCkrwLid",
    "outputId": "0b99dd26-00c8-4fdf-9db9-a627599d68cc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Модель как sequential\n",
    "model = nn.Sequential(nn.Linear(n_features, n_classes))\n",
    "\n",
    "answer = model(data)\n",
    "print(answer.shape)\n",
    "print(answer)"
   ],
   "metadata": {
    "id": "YIBvYYBewLie"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[-0.3989, -1.1825,  0.7602],\n",
      "        [-0.0798,  0.7053, -0.0033],\n",
      "        [-0.2838, -0.0166, -0.5549],\n",
      "        [-0.1167,  0.6097, -0.4273],\n",
      "        [-0.2631,  0.2675, -0.2055]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Модель как nn.ModuleList\n",
    "\n",
    "model = nn.ModuleList([nn.Linear(n_features, n_classes)])\n",
    "\n",
    "# answer = model(data)\n",
    "# print(answer.shape)\n",
    "# print(answer)\n",
    "\n",
    "answer = model[0](data)\n",
    "print(answer.shape)\n",
    "print(answer)\n"
   ],
   "metadata": {
    "id": "uBwzYlptwLif",
    "outputId": "87e96229-02eb-49ee-a608-97016ded1420"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Проверим параметры модели\n",
    "class ParametersCheck(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Linear(n_features, n_classes)\n",
    "        self.seq = nn.Sequential(nn.Linear(n_features, n_classes))\n",
    "        self.module_list = nn.ModuleList([nn.Linear(n_features, n_classes)])\n",
    "        self.list_of_layers = [nn.Linear(n_features, n_classes)]\n"
   ],
   "metadata": {
    "id": "m5fE17SRwLig"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметр #1.\n",
      "\ttorch.Size([3, 10])\n",
      "Параметр #2.\n",
      "\ttorch.Size([3])\n",
      "Параметр #3.\n",
      "\ttorch.Size([3, 10])\n",
      "Параметр #4.\n",
      "\ttorch.Size([3])\n",
      "Параметр #5.\n",
      "\ttorch.Size([3, 10])\n",
      "Параметр #6.\n",
      "\ttorch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "model = ParametersCheck(n_features, n_classes)\n",
    "\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    print(f'Параметр #{i + 1}.')\n",
    "    print(f'\\t{param.shape}')"
   ],
   "metadata": {
    "id": "pzvFgyhHwLih",
    "outputId": "e2da82be-f4b1-4749-ac11-d7256e5fc026"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ViT"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9_ccpqgpwLih"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![alt text](https://drive.google.com/uc?export=view&id=1J5TvycDPs8pzfvlXvtO5MCFBy64yp9Fa)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "O9Ck2xnvwLii"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install einops"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFzQd5YDEbas",
    "outputId": "ecdda0b7-d8e3-40b2-8e35-4fca18634e1f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.6/41.6 KB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.6.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "id": "khe7vy_ZwLii"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/vit-01.png)"
   ],
   "metadata": {
    "id": "cbPI9vsXDZH9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть 1. Patch Embedding, CLS Token, Position Encoding"
   ],
   "metadata": {
    "id": "I7Au2Fd1FZbj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/vit-02.png)"
   ],
   "metadata": {
    "id": "YjbKwA7lGY3O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# input image `B, C, H, W`\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "# 2D conv\n",
    "conv = nn.Conv2d(3, 768, 16, 16)\n",
    "conv(x).reshape(-1, 196).transpose(0,1).shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tH4Nb22GeuS",
    "outputId": "67c148e2-d216-4e7f-df44-983eac5e6a12"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([196, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        ...\n",
    "\n",
    "        self.patch_embeddings = nn. ...\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        ...\n",
    "        \n",
    "        return patches"
   ],
   "metadata": {
    "id": "WVwf4n1bwLik"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "patch_embed = PatchEmbedding()\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "patch_embed(x).shape "
   ],
   "metadata": {
    "id": "E57UzPBuE4qi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1b47e182-bcc9-4315-e457-5cbab7ef86b8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/vit-03.png)"
   ],
   "metadata": {
    "id": "JVUm-TJFGm6L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть 2. Transformer Encoder"
   ],
   "metadata": {
    "id": "rUxuB53PFv1h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/ViT.png)"
   ],
   "metadata": {
    "id": "vkklM-fqFpa9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/vit-07.png)"
   ],
   "metadata": {
    "id": "G34WzminccX7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ACAqbCivDGsa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear Layers\n",
    "        ...\n",
    "\n",
    "        # Activation(s)\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ...\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "VPQts2WWdeYQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, 197,768)\n",
    "mlp = MLP(768, 3072, 768)\n",
    "out = mlp(x)\n",
    "out.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFxxcPoMf7IW",
    "outputId": "c1f44b38-7ec4-4e93-bdb5-a4db4e116e90"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., out_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = ...\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.out = ...\n",
    "        self.out_drop = nn.Dropout(out_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Attention\n",
    "        ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        # Out projection\n",
    "\n",
    "        ...\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "4QnAW3rSc2OZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/vit-08.png)"
   ],
   "metadata": {
    "id": "S_vgvLDbcjvi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "# attn = attn.softmax(dim=-1)"
   ],
   "metadata": {
    "id": "OukFkeXzdFpB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, 197, 768)\n",
    "attention = Attention(768, 8)\n",
    "out = attention(x)\n",
    "out.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NeRHHJAgg5R",
    "outputId": "143fdcfe-b61b-47c5-efd2-e511010f43a7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalization\n",
    "        ...\n",
    "\n",
    "        # Attention\n",
    "        ...\n",
    "\n",
    "        # Dropout\n",
    "        ...\n",
    "\n",
    "        # Normalization\n",
    "        ...\n",
    "\n",
    "        # MLP\n",
    "        ...\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attetnion\n",
    "        ...\n",
    "\n",
    "        # MLP\n",
    "        ...\n",
    "        return x"
   ],
   "metadata": {
    "id": "K6e8y_YvwLik"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, 197, 768)\n",
    "block = Block(768, 8)\n",
    "out = attention(x)\n",
    "out.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aMihgfEhyql",
    "outputId": "993524e8-c7eb-4b38-802e-557e89ba8285"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В оригинальной реализации теперь используется [DropPath](https://github.com/rwightman/pytorch-image-models/blob/e98c93264cde1657b188f974dc928b9d73303b18/timm/layers/drop.py)"
   ],
   "metadata": {
    "id": "BPBmiO5FhoN6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, depth, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim, num_heads, mlp_ratio, drop_rate)\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "b1uO18VTwLil"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, 197, 768)\n",
    "block = Transformer(12, 768)\n",
    "out = attention(x)\n",
    "out.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIfp984oiBqc",
    "outputId": "704c9a49-92d6-4859-9f61-06555bf89579"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://amaarora.github.io/images/vit-06.png)"
   ],
   "metadata": {
    "id": "GqUxpyv3cwNm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., \n",
    "                 qkv_bias=False, drop_rate=0.,):\n",
    "        super().__init__()\n",
    "\n",
    "        # Присвоение переменных\n",
    "        ...\n",
    "\n",
    "        # Path Embeddings, CLS Token, Position Encoding\n",
    "        ...\n",
    "\n",
    "        # Transformer Encoder\n",
    "        ...\n",
    "\n",
    "        # Classifier\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Path Embeddings, CLS Token, Position Encoding\n",
    "        ...\n",
    "\n",
    "        # Transformer Encoder\n",
    "        ...\n",
    "\n",
    "        # Classifier\n",
    "        ...\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "Y9gyxdqQeFs6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "vit = ViT()\n",
    "out = vit(x)\n",
    "out.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lGhne8kjeYs",
    "outputId": "c660e099-46aa-4700-f5f5-25c69c8d9bda"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Домашнее задание"
   ],
   "metadata": {
    "id": "4QbFtayBkp-c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "1. Выбрать датасет для классификации изображений с размерностью 64x64+ \n",
    "2. Обучить ViT на таком датасете.\n",
    "3. Попробовать поменять размерности и посмотреть, что поменяется при обучении.\n",
    "\n",
    "\n",
    "Примечание:\n",
    "- Датасеты можно взять [тут](https://pytorch.org/vision/stable/datasets.html#built-in-datasets) или найти в другом месте.\n",
    "- Из за того, что ViT учится медленно, количество примеров в датасете можно ограничить до 1к-5к."
   ],
   "metadata": {
    "id": "6nZbwbK9kskc"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
